{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "from mnist import load_data\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training, validation, testing = load_data()\n",
    "train_x, train_y, test_x, test_y = training[0], training[1], testing[0], testing[1]\n",
    "train_y = utils.one_hot_encoding(train_y)\n",
    "test_y = utils.one_hot_encoding(test_y)\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, layers, lr=0.00001, epochs=10):\n",
    "        self.n_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        w1 = np.random.randn(784, layers[0])\n",
    "        w_last = np.random.randn(layers[len(layers)-1], 10)\n",
    "        self.weights = [np.random.randn(x, y) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        self.weights.insert(0, w1)\n",
    "        self.weights.append(w_last)\n",
    "        self.weights = np.asarray(self.weights)\n",
    "        \n",
    "        self.biases = [np.random.randn(y) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        b1 = np.random.randn(1, layers[0])\n",
    "        b_last = np.random.randn(1, 10)\n",
    "        self.biases.insert(0, b1)\n",
    "        self.biases.append(b_last)\n",
    "        self.biases = np.array(self.biases)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = 50000\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        activations = [inputs]\n",
    "        z_vec = []\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(inputs, w) + b\n",
    "            inputs = utils.sigmoid(z)\n",
    "            z_vec.append(z)\n",
    "            activations.append(inputs)\n",
    "        inputs = utils.softmax(inputs)\n",
    "        return inputs, activations, z_vec\n",
    "    \n",
    "    def compute_loss(self, logits, labels, epsilon=np.finfo(float).eps):\n",
    "        return -np.sum(np.multiply(labels, np.log10(logits+epsilon)))/logits.shape[0]\n",
    "    \n",
    "    def get_gradients(self, logits, labels, activations, z_vec):\n",
    "        nabla_w = [np.empty(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.empty(b.shape) for b in self.biases]\n",
    "        w_last = self.weights[-1]\n",
    "        error = logits - labels\n",
    "        w_last_g = np.dot(activations[-2].T, error)\n",
    "        nabla_w[-1] = w_last_g\n",
    "        error = error * utils.sigmoid_prime(z_vec[-1])\n",
    "        errors = []\n",
    "        errors.append(error)\n",
    "        for i in range(len(activations) - 3, -1, -1):\n",
    "            activation = activations[i]\n",
    "            weight = self.weights[i+1].T\n",
    "            new_error = np.dot(errors[-1], weight) * utils.sigmoid_prime(z_vec[i])\n",
    "            dB = new_error\n",
    "            dW = np.dot(activation.T, new_error)\n",
    "            nabla_w[i] = dW\n",
    "            nabla_b[i] = dB\n",
    "            errors.append(new_error)\n",
    "        return nabla_w, nabla_b\n",
    "    \n",
    "    def update_weights_and_biases(self, nabla_w, nabla_b):\n",
    "        for dW in nabla_w:\n",
    "            dW *= self.lr/self.batch_size\n",
    "        for dB in nabla_b:\n",
    "            dB *- self.lr/self.batch_size\n",
    "        self.weights -= nabla_w\n",
    "        self.biases -= nabla_b\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.epochs):\n",
    "            logits, activations, z = self.feed_forward(train_x)\n",
    "            nabla_w, nabla_b = self.get_gradients(logits=logits, labels=train_y, activations=activations, z_vec=z)\n",
    "            self.update_weights_and_biases(nabla_w, nabla_b)\n",
    "    \n",
    "    def test(self):\n",
    "        for i in range(5):\n",
    "            logits, activations, z = self.feed_forward(test_x[i])\n",
    "            prediction = np.argmax(logits, axis=1)\n",
    "            print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.39731197e-06 2.06357817e-06 1.91964464e-06 ... 2.36678569e-06\n",
      "  2.02681642e-06 2.19020918e-06]\n",
      " [2.41283494e-06 1.95849668e-06 2.02677022e-06 ... 2.35136095e-06\n",
      "  2.07366892e-06 2.21471978e-06]\n",
      " [2.40193194e-06 2.03554605e-06 1.97593215e-06 ... 2.34866356e-06\n",
      "  2.03644556e-06 2.19774093e-06]\n",
      " ...\n",
      " [2.40225201e-06 2.00591946e-06 1.97573306e-06 ... 2.33927564e-06\n",
      "  2.10016004e-06 2.16329584e-06]\n",
      " [2.40187598e-06 2.03650918e-06 1.97506326e-06 ... 2.34763048e-06\n",
      "  2.04071241e-06 2.19547451e-06]\n",
      " [2.40220847e-06 2.01264269e-06 1.97194872e-06 ... 2.33884800e-06\n",
      "  2.09785627e-06 2.15729624e-06]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[2.08217695e-06 2.16059593e-06 2.13219900e-06 ... 2.42091853e-06\n",
      "  1.70241381e-06 1.87953090e-06]\n",
      " [2.24613769e-06 2.08367819e-06 2.20759708e-06 ... 2.41970615e-06\n",
      "  1.74938024e-06 1.93913173e-06]\n",
      " [2.11042975e-06 2.13964375e-06 2.18201165e-06 ... 2.41976857e-06\n",
      "  1.72677906e-06 1.89079865e-06]\n",
      " ...\n",
      " [2.12003886e-06 2.12650120e-06 2.18081220e-06 ... 2.41923884e-06\n",
      "  1.79027993e-06 1.84260871e-06]\n",
      " [2.10981008e-06 2.14273088e-06 2.18341410e-06 ... 2.41974920e-06\n",
      "  1.72777228e-06 1.88768438e-06]\n",
      " [2.11559979e-06 2.13380868e-06 2.18185976e-06 ... 2.41923851e-06\n",
      "  1.78586472e-06 1.83442155e-06]]\n",
      "[7 7 7 ... 7 7 7]\n",
      "[[2.45032643e-06 2.15597810e-06 1.82516816e-06 ... 2.37335309e-06\n",
      "  2.02200712e-06 2.21033254e-06]\n",
      " [2.47409579e-06 2.03354034e-06 1.96364252e-06 ... 2.34332897e-06\n",
      "  2.06701630e-06 2.26930152e-06]\n",
      " [2.45602740e-06 2.13313091e-06 1.89192449e-06 ... 2.33586045e-06\n",
      "  2.04042687e-06 2.22462559e-06]\n",
      " ...\n",
      " [2.45673460e-06 2.11441715e-06 1.89217950e-06 ... 2.32241034e-06\n",
      "  2.10233738e-06 2.18876146e-06]\n",
      " [2.45591354e-06 2.13473216e-06 1.89275426e-06 ... 2.33483064e-06\n",
      "  2.04366744e-06 2.22167657e-06]\n",
      " [2.45626639e-06 2.12013752e-06 1.88820397e-06 ... 2.32105970e-06\n",
      "  2.10146820e-06 2.17931204e-06]]\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[2.26623902e-06 2.05282820e-06 1.99949529e-06 ... 2.41780375e-06\n",
      "  1.78160916e-06 1.93360827e-06]\n",
      " [2.38448621e-06 1.97072805e-06 2.10057193e-06 ... 2.40891918e-06\n",
      "  1.89831606e-06 2.04369031e-06]\n",
      " [2.27849632e-06 2.02043774e-06 2.04586767e-06 ... 2.41370880e-06\n",
      "  1.85233832e-06 1.94289297e-06]\n",
      " ...\n",
      " [2.28519826e-06 1.98383234e-06 2.04427377e-06 ... 2.41085716e-06\n",
      "  1.95056255e-06 1.89567189e-06]\n",
      " [2.27995974e-06 2.02073215e-06 2.04523491e-06 ... 2.41337300e-06\n",
      "  1.85702375e-06 1.94216025e-06]\n",
      " [2.28315200e-06 1.99478331e-06 2.04125452e-06 ... 2.41088680e-06\n",
      "  1.93864664e-06 1.87654201e-06]]\n",
      "[7 7 7 ... 7 7 7]\n",
      "[[2.19987052e-06 2.06627363e-06 2.15970035e-06 ... 2.32003688e-06\n",
      "  1.92412400e-06 1.97883039e-06]\n",
      " [2.28028103e-06 2.02542549e-06 2.18740529e-06 ... 2.31232321e-06\n",
      "  1.97182297e-06 2.06924134e-06]\n",
      " [2.21283879e-06 2.04490630e-06 2.18060275e-06 ... 2.31675952e-06\n",
      "  1.93735944e-06 2.00722381e-06]\n",
      " ...\n",
      " [2.21702521e-06 2.02110752e-06 2.17724678e-06 ... 2.31463097e-06\n",
      "  2.00314203e-06 1.98062649e-06]\n",
      " [2.21602570e-06 2.04696658e-06 2.18006593e-06 ... 2.31628635e-06\n",
      "  1.94309387e-06 2.00865103e-06]\n",
      " [2.21763831e-06 2.03336994e-06 2.17729851e-06 ... 2.31456928e-06\n",
      "  1.99687032e-06 1.96922468e-06]]\n",
      "[7 7 7 ... 7 7 7]\n"
     ]
    }
   ],
   "source": [
    "model = Network([10, 20])\n",
    "model.train()\n",
    "model.test()\n",
    "t = np.array([1, 2, 3, 4, 5, 6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
