{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import numpy as np\n",
    "from mnist import load_data\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training, validation, testing = load_data()\n",
    "train_x, train_y, test_x, test_y = training[0], training[1], testing[0], testing[1]\n",
    "train_y = utils.one_hot_encoding(train_y)\n",
    "test_y = utils.one_hot_encoding(test_y)\n",
    "\n",
    "class Network():\n",
    "    def __init__(self, layers, lr=0.0001, epochs=10):\n",
    "        self.n_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        w1 = np.random.rand(784, layers[0])\n",
    "        w_last = np.random.rand(layers[len(layers)-1], 10)\n",
    "        self.weights = [np.random.rand(x, y) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        self.weights.insert(0, w1)\n",
    "        self.weights.append(w_last)\n",
    "        self.weights = np.asarray(self.weights)\n",
    "        \n",
    "        self.biases = [np.random.rand(y) for x, y in zip(layers[:-1], layers[1:])]\n",
    "        b1 = np.random.rand(1, layers[0])\n",
    "        b_last = np.random.rand(1, 10)\n",
    "        self.biases.insert(0, b1)\n",
    "        self.biases.append(b_last)\n",
    "        self.biases = np.array(self.biases)\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def feed_forward(self, inputs):\n",
    "        activations = [inputs]\n",
    "        z_vec = []\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(inputs, w) + b\n",
    "            inputs = utils.sigmoid(z)\n",
    "            print(inputs)\n",
    "            z_vec.append(z)\n",
    "            activations.append(inputs)\n",
    "        inputs = utils.softmax(inputs)\n",
    "        return inputs, activations, z_vec\n",
    "    \n",
    "    def compute_loss(self, logits, labels, epsilon=np.finfo(float).eps):\n",
    "        return -np.sum(np.multiply(labels, np.log10(logits+epsilon)))/logits.shape[0]\n",
    "    \n",
    "    def get_gradients(self, logits, labels, activations, z_vec):\n",
    "        nabla_w = [np.empty(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.empty(b.shape) for b in self.biases]\n",
    "        w_last = self.weights[-1]\n",
    "        error = logits - labels\n",
    "        w_last_g = np.dot(activations[-2].T, error)\n",
    "        nabla_w[-1] = w_last_g\n",
    "        error = error * utils.sigmoid_prime(z_vec[-1])\n",
    "        errors = []\n",
    "        errors.append(error)\n",
    "        for i in range(len(activations) - 3, -1, -1):\n",
    "            activation = activations[i]\n",
    "            weight = self.weights[i+1].T\n",
    "            new_error = np.dot(errors[-1], weight) * utils.sigmoid_prime(z_vec[i])\n",
    "            dB = new_error\n",
    "            dW = np.dot(activation.T, new_error)\n",
    "            nabla_w[i] = dW\n",
    "            nabla_b[i] = dB\n",
    "            errors.append(new_error)\n",
    "        return nabla_w, nabla_b\n",
    "    \n",
    "    def update_weights_and_biases(self, nabla_w, nabla_b):\n",
    "        for dW in nabla_w:\n",
    "            dW *= self.lr\n",
    "        for dB in nabla_b:\n",
    "            dB *- self.lr\n",
    "        self.weights -= nabla_w\n",
    "        self.biases -= nabla_b\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(self.epochs):\n",
    "            logits, activations, z = self.feed_forward(train_x)\n",
    "            nabla_w, nabla_b = self.get_gradients(logits=logits, labels=train_y, activations=activations, z_vec=z)\n",
    "            self.update_weights_and_biases(nabla_w, nabla_b)\n",
    "    \n",
    "    def test(self):\n",
    "        for i in range(5):\n",
    "#             print(test_x[i])\n",
    "            logits, activations, z = self.feed_forward(test_x[i])\n",
    "            prediction = np.argmax(logits)\n",
    "#             print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0.97651505 0.99859027 0.99612946 0.99656451 0.99840389 0.99891232\n",
      "  0.99766194 0.99507908 0.99511194 0.98022623 0.99140057 0.99757099\n",
      "  0.99097223 0.99873065 0.99468915 0.99494115 0.98336007 0.9971502\n",
      "  0.99645053 0.9925074 ]]\n",
      "[[0.99998436 0.9999826  0.99950704 0.999923   0.99987839 0.99992388\n",
      "  0.99999096 0.99972664 0.99992476 0.99997646]]\n",
      "3\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0.97651505 0.99859027 0.99612946 0.99656451 0.99840389 0.99891232\n",
      "  0.99766194 0.99507908 0.99511194 0.98022623 0.99140057 0.99757099\n",
      "  0.99097223 0.99873065 0.99468915 0.99494115 0.98336007 0.9971502\n",
      "  0.99645053 0.9925074 ]]\n",
      "[[0.99998436 0.9999826  0.99950704 0.999923   0.99987839 0.99992388\n",
      "  0.99999096 0.99972664 0.99992476 0.99997646]]\n",
      "3\n",
      "[[0.99999999 1.         1.         0.99999999 1.         1.\n",
      "  1.         1.         1.         0.99999999]]\n",
      "[[0.97651505 0.99859027 0.99612946 0.99656451 0.99840389 0.99891232\n",
      "  0.99766194 0.99507908 0.99511194 0.98022623 0.99140057 0.99757099\n",
      "  0.99097223 0.99873065 0.99468915 0.99494115 0.98336007 0.9971502\n",
      "  0.99645053 0.9925074 ]]\n",
      "[[0.99998436 0.9999826  0.99950704 0.999923   0.99987839 0.99992388\n",
      "  0.99999096 0.99972664 0.99992476 0.99997646]]\n",
      "3\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0.97651505 0.99859027 0.99612946 0.99656451 0.99840389 0.99891232\n",
      "  0.99766194 0.99507908 0.99511194 0.98022623 0.99140057 0.99757099\n",
      "  0.99097223 0.99873065 0.99468915 0.99494115 0.98336007 0.9971502\n",
      "  0.99645053 0.9925074 ]]\n",
      "[[0.99998436 0.9999826  0.99950704 0.999923   0.99987839 0.99992388\n",
      "  0.99999096 0.99972664 0.99992476 0.99997646]]\n",
      "3\n",
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "[[0.97651505 0.99859027 0.99612946 0.99656451 0.99840389 0.99891232\n",
      "  0.99766194 0.99507908 0.99511194 0.98022623 0.99140057 0.99757099\n",
      "  0.99097223 0.99873065 0.99468915 0.99494115 0.98336007 0.9971502\n",
      "  0.99645053 0.9925074 ]]\n",
      "[[0.99998436 0.9999826  0.99950704 0.999923   0.99987839 0.99992388\n",
      "  0.99999096 0.99972664 0.99992476 0.99997646]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "model = Network([10, 20])\n",
    "# model.train()\n",
    "model.test()\n",
    "t = np.array([1, 2, 3, 4, 5, 6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
